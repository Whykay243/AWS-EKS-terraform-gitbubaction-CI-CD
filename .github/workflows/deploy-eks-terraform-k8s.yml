name: Deploy EKS via Terraform and Kubernetes

on:
  push:
    branches: [main]

permissions:
  id-token: write          # allow OIDC role assumption
  contents: read

env:
  AWS_REGION: us-east-1                # AWS region to use
  BUCKET_NAME: whykay-backend-bucket   # S3 bucket for Terraform backend
  TF_KEY: eks/terraform.tfstate        # backend key path
  TF_REGION: us-east-1                 # backend region
  CLUSTER_NAME: prod-eks-cluster       # EKS cluster name (adjust if needed)
  NODEGROUP_PREFIX: worker-nodes       # node group prefix (used to detect full name)

jobs:
  terraform:
    name: Terraform Plan & Apply
    runs-on: ubuntu-latest
    outputs:
      nodegroup: ${{ steps.get-nodegroup.outputs.nodegroup }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        # get repository code so Terraform can run against the configs

      - name: Configure AWS credentials using OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::293926504711:role/Githubaction-aws-assumerole
          aws-region: ${{ env.AWS_REGION }}
        # Authenticate to AWS using GitHub OIDC (no static keys required)

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        # install Terraform CLI

      - name: Terraform Init
        run: |
          terraform init \
            -backend-config="bucket=${{ env.BUCKET_NAME }}" \
            -backend-config="key=${{ env.TF_KEY }}" \
            -backend-config="region=${{ env.TF_REGION }}"
        # init with S3 backend config (state stored remotely)

      - name: Terraform Validate
        run: terraform validate
        # quick syntax/config check

      - name: Terraform Plan
        run: terraform plan -out=tfplan.out
        # generate plan (saved to file to apply exactly)

      - name: Terraform Apply
        if: github.ref == 'refs/heads/main'
        run: terraform apply -auto-approve tfplan.out
        # apply resources only on main branch

      - name: Wait for EKS cluster to be active
        if: github.ref == 'refs/heads/main'
        run: |
          echo "Waiting for EKS cluster '${{ env.CLUSTER_NAME }}' to become ACTIVE..."
          aws eks wait cluster-active --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
          echo "EKS cluster is ACTIVE.✅"
        # ensure EKS control plane is ready before continuing

      - name: List existing node groups for debugging
        if: github.ref == 'refs/heads/main'
        run: |
          echo "Listing current node groups for cluster '${{ env.CLUSTER_NAME }}'..."
          aws eks list-nodegroups --cluster-name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
        # debug: check what node groups currently exist

      - name: Get full node group name dynamically
        id: get-nodegroup
        if: github.ref == 'refs/heads/main'
        run: |
          nodegroup_full_name=$(aws eks list-nodegroups \
            --cluster-name ${{ env.CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --query "nodegroups[?starts_with(@, '${{ env.NODEGROUP_PREFIX }}')]" \
            --output text)
          echo "Found nodegroup: $nodegroup_full_name"
          echo "nodegroup=$nodegroup_full_name" >> $GITHUB_OUTPUT

        # dynamically find node group name that starts with the prefix

      - name: Wait 60 seconds to allow node group creation
        if: github.ref == 'refs/heads/main'
        run: sleep 60
        # wait a bit before checking node group status to avoid timing issues

      - name: Wait for node group to be active
        if: github.ref == 'refs/heads/main'
        run: |
          nodegroup=${{ steps.get-nodegroup.outputs.nodegroup }}
          echo "Waiting for nodegroup '$nodegroup' to be ACTIVE..."
          aws eks wait nodegroup-active \
            --cluster-name ${{ env.CLUSTER_NAME }} \
            --nodegroup-name "$nodegroup" \
            --region ${{ env.AWS_REGION }}
          echo "Node group is ACTIVE.✅"
        # ensure node group (worker nodes) is ready so workloads can schedule

      - name: Generate kubeconfig file dynamically
        if: github.ref == 'refs/heads/main'
        run: |
          aws eks update-kubeconfig \
            --name ${{ env.CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --kubeconfig kubeconfig.yaml
        # create kubeconfig for subsequent kubectl/helm steps

      - name: Upload kubeconfig artifact (optional)
        if: github.ref == 'refs/heads/main'
        uses: actions/upload-artifact@v4
        with:
          name: kubeconfig
          path: kubeconfig.yaml
        # upload kubeconfig as an artifact for manual download or later jobs

      # - name: Terraform Destroy
      #   run: terraform destroy -auto-approve
